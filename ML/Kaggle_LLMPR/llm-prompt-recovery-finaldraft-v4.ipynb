{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3889080",
   "metadata": {
    "papermill": {
     "duration": 0.00944,
     "end_time": "2024-04-16T02:55:38.550115",
     "exception": false,
     "start_time": "2024-04-16T02:55:38.540675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LLM Prompt Recovery - Final Draftv4\n",
    "\n",
    "## General idea:\n",
    "\n",
    "\n",
    "1) If the new and old text are \"too similar\" then we say it is just \"Improve this text\" or whatever\n",
    "\n",
    "2) Use Mistral or something to classify all of the new and old texts\n",
    "\n",
    "3) Use that information to put it into a cluster\n",
    "\n",
    "4) From that cluster, generate an average representational prompt\n",
    "\n",
    "Thanks to Rich Olson for providing some of the formatting code in his Mistral notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7adb8a29",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-16T02:55:38.572402Z",
     "iopub.status.busy": "2024-04-16T02:55:38.571591Z",
     "iopub.status.idle": "2024-04-16T02:58:10.031923Z",
     "shell.execute_reply": "2024-04-16T02:58:10.030607Z"
    },
    "papermill": {
     "duration": 151.474143,
     "end_time": "2024-04-16T02:58:10.034636",
     "exception": false,
     "start_time": "2024-04-16T02:55:38.560493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pip install\n",
    "!pip install -q -U \\\n",
    "    bitsandbytes peft accelerate \\\n",
    "    transformers unsloth triton xformers torch \\\n",
    "    --no-index \\\n",
    "    --find-links \\\n",
    "    /kaggle/input/prompt-recover-pip-wheels/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277308d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T02:58:10.053397Z",
     "iopub.status.busy": "2024-04-16T02:58:10.053059Z",
     "iopub.status.idle": "2024-04-16T02:58:37.484952Z",
     "shell.execute_reply": "2024-04-16T02:58:37.483940Z"
    },
    "papermill": {
     "duration": 27.444041,
     "end_time": "2024-04-16T02:58:37.487407",
     "exception": false,
     "start_time": "2024-04-16T02:58:10.043366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 02:58:23.342262: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-16 02:58:23.342393: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-16 02:58:23.604485: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "import accelerate\n",
    "from datasets import load_dataset, Dataset\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    PeftModel,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "#if (not torch.cuda.is_available()): print(\"Cuda not working/GPU not detected\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21355882",
   "metadata": {
    "papermill": {
     "duration": 0.00818,
     "end_time": "2024-04-16T02:58:37.504739",
     "exception": false,
     "start_time": "2024-04-16T02:58:37.496559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Mean prompt setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d8e640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T02:58:37.524282Z",
     "iopub.status.busy": "2024-04-16T02:58:37.523641Z",
     "iopub.status.idle": "2024-04-16T02:58:37.528452Z",
     "shell.execute_reply": "2024-04-16T02:58:37.527610Z"
    },
    "papermill": {
     "duration": 0.01647,
     "end_time": "2024-04-16T02:58:37.530755",
     "exception": false,
     "start_time": "2024-04-16T02:58:37.514285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_prefix = \"Please improve the following text by reimagining it through the lens of a\"\n",
    "response_suffix = \", retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\"\n",
    "base_response=\"Please improve the following text by reimagining it through the lens of a , retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bdc83c",
   "metadata": {
    "papermill": {
     "duration": 0.008372,
     "end_time": "2024-04-16T02:58:37.548238",
     "exception": false,
     "start_time": "2024-04-16T02:58:37.539866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loading Competition Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2861f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T02:58:37.566957Z",
     "iopub.status.busy": "2024-04-16T02:58:37.566694Z",
     "iopub.status.idle": "2024-04-16T02:58:37.896902Z",
     "shell.execute_reply": "2024-04-16T02:58:37.895971Z"
    },
    "papermill": {
     "duration": 0.342829,
     "end_time": "2024-04-16T02:58:37.899551",
     "exception": false,
     "start_time": "2024-04-16T02:58:37.556722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id                                      original_text  \\\n",
      "1419  llsUqKWcIj  `` Huh... whats this for?'' I say as I plug th...   \n",
      "556   XkJfBmJXkI  Death extended her hand to me `` deal?'' she a...   \n",
      "1879  vWEqkRWQCt  Three men rounded the corner carrying carrying...   \n",
      "1430  KZkoeukpMZ  I picked up my newspaper. And read the headlin...   \n",
      "2370  GOlHSsFDJc  “ We are very proud of what you did. Your rese...   \n",
      "\n",
      "                                         rewrite_prompt  \\\n",
      "1419  Rewrite the story as a modern-day dystopian novel   \n",
      "556   Rewrite the essay as a poem with strong rhythm...   \n",
      "1879  Rewrite the essay to make the act deliberate, ...   \n",
      "1430  Rewrite the story as if they are both doing it...   \n",
      "2370  Rewrite the story with more humor and visual i...   \n",
      "\n",
      "                                         rewritten_text  rewrite_blank  \n",
      "1419  In the dystopian wasteland of the future, huma...          False  \n",
      "556   Death's hand extended, a deal to make,\\nA smil...          False  \n",
      "1879  Three men rounded the corner carrying large su...          False  \n",
      "1430  I picked up my newspaper, and read the headlin...          False  \n",
      "2370  “My alien visitor, with his wiry voice and met...          False  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
    "else:\n",
    "    #test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
    "    test_df = pd.read_csv(\"/kaggle/input/gemma-rewrite-nbroad/nbroad-v2.csv\")\n",
    "    #test_df=pd.read_csv(\"/kaggle/input/combined-results/combined_results_a.csv\")\n",
    "    test_df=test_df.sample(20)\n",
    "    #test_df['rewritten_text']=test_df['only_rewritten']\n",
    "    #test_df=test_df.drop(['only_rewritten','has_colon', 'colon_snip_before','colon_snip_after','date','placeholder'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_df['rewrite_blank']=False\n",
    "test_df['rewrite_blank']=test_df['rewritten_text'].isna()\n",
    "test_df['original_text'] = test_df['original_text'].fillna(' ')\n",
    "test_df['rewritten_text'] = test_df['rewritten_text'].fillna(' ')\n",
    "testdf_copy=test_df.copy(deep=True)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b80a095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T02:58:37.919418Z",
     "iopub.status.busy": "2024-04-16T02:58:37.919093Z",
     "iopub.status.idle": "2024-04-16T02:58:37.923436Z",
     "shell.execute_reply": "2024-04-16T02:58:37.922647Z"
    },
    "papermill": {
     "duration": 0.016009,
     "end_time": "2024-04-16T02:58:37.925378",
     "exception": false,
     "start_time": "2024-04-16T02:58:37.909369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_texts=test_df['original_text'].tolist()\n",
    "rewritten_texts=test_df['rewritten_text'].tolist()\n",
    "reslist=[]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fa3368",
   "metadata": {
    "papermill": {
     "duration": 0.008271,
     "end_time": "2024-04-16T02:58:37.942045",
     "exception": false,
     "start_time": "2024-04-16T02:58:37.933774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load Mistral\n",
    "This is going to use the finetuned unsloth mistral-7b-bnb-4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50d1f121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T02:58:37.960214Z",
     "iopub.status.busy": "2024-04-16T02:58:37.959944Z",
     "iopub.status.idle": "2024-04-16T02:59:47.882073Z",
     "shell.execute_reply": "2024-04-16T02:59:47.881094Z"
    },
    "papermill": {
     "duration": 69.933933,
     "end_time": "2024-04-16T02:59:47.884611",
     "exception": false,
     "start_time": "2024-04-16T02:58:37.950678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='unsloth/mistral-7b-instruct-v0.2-bnb-4bit', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=32, target_modules={'model.layers.31.self_attn.q_proj', 'model.layers.31.self_attn.o_proj', 'model.layers.31.self_attn.k_proj', 'model.layers.31.mlp.gate_proj', 'model.layers.31.mlp.down_proj', 'model.layers.31.mlp.up_proj', 'model.layers.31.self_attn.v_proj'}, lora_alpha=128, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"unsloth/mistral-7b-bnb-4bit\"\n",
    "\n",
    "PEFT_MODEL = \"/kaggle/input/mistral-7b-it-bnb-4bit-finetuned-text\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "print(config)\n",
    "#print(config.base_model_name_or_path)\n",
    "base_model_path=\"/kaggle/input/custom-mistral-inference/unsloth\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)\n",
    "\n",
    "\n",
    "\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 42\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "generation_config_tt = model.generation_config\n",
    "generation_config_tt.max_new_tokens = 6\n",
    "generation_config_tt.temperature = 0.7\n",
    "generation_config_tt.top_p = 0.7\n",
    "generation_config_tt.num_return_sequences = 1\n",
    "generation_config_tt.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config_tt.eos_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4df8b",
   "metadata": {
    "papermill": {
     "duration": 0.008458,
     "end_time": "2024-04-16T02:59:47.902416",
     "exception": false,
     "start_time": "2024-04-16T02:59:47.893958",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Chat template Model for Mistral\n",
    "\n",
    "Here we'll set up the chat templates for text-type inference, colon examples, and prompt rewrite responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95650696",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T02:59:47.920610Z",
     "iopub.status.busy": "2024-04-16T02:59:47.920304Z",
     "iopub.status.idle": "2024-04-16T02:59:47.934679Z",
     "shell.execute_reply": "2024-04-16T02:59:47.933590Z"
    },
    "papermill": {
     "duration": 0.025682,
     "end_time": "2024-04-16T02:59:47.936519",
     "exception": false,
     "start_time": "2024-04-16T02:59:47.910837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'].strip() + '\\n\\n' %}{% else %}{% set loop_messages = messages %}{% set system_message = '' %}{% endif %}{{ bos_token }}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 %}{% set content = system_message + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n"
     ]
    }
   ],
   "source": [
    "chat_template = open('/kaggle/input/mistral-instruct-templ/mistral-instruct.jinja').read()\n",
    "chat_template = chat_template.replace('    ', '').replace('\\n', '')\n",
    "tokenizer.chat_template=chat_template\n",
    "print(tokenizer.chat_template)\n",
    "\n",
    "sys_mes_text_inf=\"\"\"Context:You are an assistant that is helping me classify types of text. Please give a one or two word response indicating the category type of text. The list of possible words are: [\"Article\",\"BlogPost\",\"Diary\",\"Essay\",\"Instructions\",\"Letter\",\"Memos\",\"Newsletters\",\"NewslettersReviews\",\"Poetry\",\"Postcards\",\"Presentations\",\"Profile\",\"Proposal\",\"CinemaScripted\",\"CookingRecipes\",\"Review\",\"Speeches\",\"Storytelling\",\"Tweets\"].Use only one category in your reply!\"\"\"\n",
    "\n",
    "sys_mes_prompt_rewrite=\"\"\"Context:You are an assistant that is helping derive rewrite prompts from the original text and the rewritten text. Please do your best to guess at what the rewrite prompt is from how the original text changes into the rewritten text!\"\"\"\n",
    "device=\"cuda\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc03f9cf",
   "metadata": {
    "papermill": {
     "duration": 0.008338,
     "end_time": "2024-04-16T02:59:47.953654",
     "exception": false,
     "start_time": "2024-04-16T02:59:47.945316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prompt for text-type inference\n",
    "Cluster based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d51165cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T02:59:47.971920Z",
     "iopub.status.busy": "2024-04-16T02:59:47.971627Z",
     "iopub.status.idle": "2024-04-16T02:59:47.985134Z",
     "shell.execute_reply": "2024-04-16T02:59:47.984486Z"
    },
    "papermill": {
     "duration": 0.024865,
     "end_time": "2024-04-16T02:59:47.987047",
     "exception": false,
     "start_time": "2024-04-16T02:59:47.962182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_prompts=pd.read_csv(\"/kaggle/input/clusterprompts/cluster_prompts.csv\")\n",
    "cluster_dict=dict()\n",
    "for ind,row in cluster_prompts.iterrows():\n",
    "    cluster_name=row[\"Cluster\"]\n",
    "    mean_prompt=row[\"Mean_Prompt\"]\n",
    "    cluster_dict.update({cluster_name:mean_prompt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728f34b",
   "metadata": {
    "papermill": {
     "duration": 0.008377,
     "end_time": "2024-04-16T02:59:48.004507",
     "exception": false,
     "start_time": "2024-04-16T02:59:47.996130",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb97cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T02:59:48.022672Z",
     "iopub.status.busy": "2024-04-16T02:59:48.022423Z",
     "iopub.status.idle": "2024-04-16T02:59:48.026162Z",
     "shell.execute_reply": "2024-04-16T02:59:48.025316Z"
    },
    "papermill": {
     "duration": 0.014852,
     "end_time": "2024-04-16T02:59:48.027974",
     "exception": false,
     "start_time": "2024-04-16T02:59:48.013122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_response=\"Please improve this text using the writing style with maintaining the original meaning but altering the tone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7201d95b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T02:59:48.046301Z",
     "iopub.status.busy": "2024-04-16T02:59:48.045814Z",
     "iopub.status.idle": "2024-04-16T02:59:48.050234Z",
     "shell.execute_reply": "2024-04-16T02:59:48.049496Z"
    },
    "papermill": {
     "duration": 0.015605,
     "end_time": "2024-04-16T02:59:48.052154",
     "exception": false,
     "start_time": "2024-04-16T02:59:48.036549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_inf_mean_prompt(text_type_a,text_type_b):\n",
    "    try:\n",
    "        mp=cluster_dict[text_type_b]\n",
    "    except:\n",
    "        print(text_type_b)\n",
    "        print(\"OH NO!\")\n",
    "        mp=base_response\n",
    "    return mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "938406bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T02:59:48.070657Z",
     "iopub.status.busy": "2024-04-16T02:59:48.070353Z",
     "iopub.status.idle": "2024-04-16T02:59:48.076558Z",
     "shell.execute_reply": "2024-04-16T02:59:48.075722Z"
    },
    "papermill": {
     "duration": 0.017747,
     "end_time": "2024-04-16T02:59:48.078435",
     "exception": false,
     "start_time": "2024-04-16T02:59:48.060688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_type_m(text):\n",
    "    text_cat = text.split(\"[/INST] \")[1]\n",
    "    text_cat.replace(\" \", \"\")\n",
    "    best=9999\n",
    "    best_word=\"BLAH\"\n",
    "    for word in [\"Article\",\"BlogPost\",\"Diary\",\"Essay\",\"Instructions\",\n",
    "                 \"Letter\",\"Memos\",\"Newsletters\",\"NewslettersReviews\",\n",
    "                 \"Poetry\",\"Postcards\",\"Presentations\",\"Profile\",\"Proposal\",\n",
    "                 \"CinemaScripted\",\"CookingRecipes\",\"Review\",\"Speeches\",\n",
    "                 \"Storytelling\",\"Tweets\"]:\n",
    "        num = text_cat.find(word)\n",
    "        if (num < best) and (num>=0):\n",
    "            best=num\n",
    "            best_word=word\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d489c5",
   "metadata": {
    "papermill": {
     "duration": 0.008217,
     "end_time": "2024-04-16T02:59:48.095304",
     "exception": false,
     "start_time": "2024-04-16T02:59:48.087087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate approximate prompts\n",
    "\n",
    "Here we want to efficiently generate approximate prompts for all 3 cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8872dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T02:59:48.113338Z",
     "iopub.status.busy": "2024-04-16T02:59:48.113068Z",
     "iopub.status.idle": "2024-04-16T03:03:16.766894Z",
     "shell.execute_reply": "2024-04-16T03:03:16.765747Z"
    },
    "papermill": {
     "duration": 208.665428,
     "end_time": "2024-04-16T03:03:16.769114",
     "exception": false,
     "start_time": "2024-04-16T02:59:48.103686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419\n",
      "BLAH\n",
      "Storytelling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556\n",
      "Storytelling\n",
      "Poetry\n",
      "1879\n",
      "Storytelling\n",
      "Storytelling\n",
      "1430\n",
      "BLAH\n",
      "Newsletters\n",
      "2370\n",
      "Newsletters\n",
      "BLAH\n",
      "2\n",
      "Letter\n",
      "Letter\n",
      "819\n",
      "Storytelling\n",
      "Storytelling\n",
      "2341\n",
      "Storytelling\n",
      "Storytelling\n",
      "1862\n",
      "Storytelling\n",
      "BLAH\n",
      "940\n",
      "Storytelling\n",
      "Storytelling\n",
      "1868\n",
      "Storytelling\n",
      "Storytelling\n",
      "630\n",
      "Articles\n",
      "Storytelling\n",
      "1456\n",
      "Storytelling\n",
      "Storytelling\n",
      "343\n",
      "Storytelling\n",
      "Storytelling\n",
      "194\n",
      "Newsletters\n",
      "BLAH\n",
      "2239\n",
      "Poetry\n",
      "BLAH\n",
      "963\n",
      "Storytelling\n",
      "Storytelling\n",
      "1328\n",
      "Speeches\n",
      "Storytelling\n",
      "2138\n",
      "Storytelling\n",
      "Storytelling\n",
      "483\n",
      "Articles\n",
      "Poetry\n"
     ]
    }
   ],
   "source": [
    "messages=[{\"role\":\"system\", \"content\": sys_mes_text_inf}]\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    if row['rewrite_blank']:\n",
    "        result = base_response\n",
    "        test_df.at[index, 'rewrite_prompt'] = result\n",
    "        test_df.at[index,'done']=True\n",
    "        continue\n",
    "    else:\n",
    "        original_text=row['original_text']\n",
    "        rewritten_text=row['rewritten_text']\n",
    "        \n",
    "        prompty=f\"\"\"Text:{original_text} Category:\"\"\"\n",
    "        new_messages=[{\"role\":\"system\", \"content\": sys_mes_text_inf},{\"role\":\"user\",\"content\":prompty}]\n",
    "        encoding = tokenizer.apply_chat_template(new_messages,tokenize=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.inference_mode():\n",
    "          outputs = model.generate(\n",
    "             encoding,\n",
    "              generation_config = generation_config_tt\n",
    "          )\n",
    "        str_out=tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "        og_m_tt=text_type_m(str_out)\n",
    "        prompty=f\"\"\"Text:{rewritten_text} Category:\"\"\"\n",
    "        new_messages=[{\"role\":\"system\", \"content\": sys_mes_text_inf},{\"role\":\"user\",\"content\":prompty}]\n",
    "        encoding = tokenizer.apply_chat_template(new_messages,tokenize=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.inference_mode():\n",
    "          outputs = model.generate(\n",
    "             encoding,\n",
    "              generation_config = generation_config_tt\n",
    "          )\n",
    "        str_out=tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "        rw_m_tt=text_type_m(str_out)\n",
    "\n",
    "\n",
    "        if og_m_tt==\"Blog post\":\n",
    "            og_m_tt=\"BlogPost\"\n",
    "        if rw_m_tt==\"Blog post\":\n",
    "            rw_m_tt=\"BlogPost\"\n",
    "        if og_m_tt==\"Cinema Scripted\":\n",
    "            og_m_tt=\"CinemaScripted\"\n",
    "        if rw_m_tt==\"Cinema Scripted\":\n",
    "            rw_m_tt=\"CinemaScripted\"\n",
    "        if og_m_tt==\"Recipes\":\n",
    "            og_m_tt=\"CookingRecipes\"\n",
    "        if rw_m_tt==\"Recipes\":\n",
    "            rw_m_tt=\"CookingRecipes\"\n",
    "        if og_m_tt==\"Article\":\n",
    "            og_m_tt=\"Articles\"\n",
    "        if rw_m_tt==\"Article\":\n",
    "            rw_m_tt=\"Articles\"\n",
    "        if og_m_tt==\"Diary\":\n",
    "            og_m_tt=\"Dairy\"\n",
    "        if rw_m_tt==\"Diary\":\n",
    "            rw_m_tt=\"Dairy\"\n",
    "        if og_m_tt==\"Business Proposal\":\n",
    "            og_m_tt=\"Proposal\"\n",
    "        if rw_m_tt==\"Business Proposal\":\n",
    "            rw_m_tt=\"Proposal\"\n",
    "        if og_m_tt==\"Newsletter Reviews\":\n",
    "            og_m_tt=\"NewslettersReviews\"\n",
    "        if rw_m_tt==\"Newsletter Reviews\":\n",
    "            rw_m_tt=\"NewslettersReviews\"\n",
    "        print(f\"{index}\")\n",
    "        print(og_m_tt)\n",
    "        print(rw_m_tt)\n",
    "        if rw_m_tt!=\"BLAH\": \n",
    "            if og_m_tt!=rw_m_tt:\n",
    "                result=text_inf_mean_prompt(og_m_tt,rw_m_tt)\n",
    "                test_df.at[index, 'rewrite_prompt'] = result\n",
    "                test_df.at[index,'done']=True\n",
    "                continue\n",
    "                \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b6ac9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T03:03:16.791925Z",
     "iopub.status.busy": "2024-04-16T03:03:16.791488Z",
     "iopub.status.idle": "2024-04-16T03:03:21.829127Z",
     "shell.execute_reply": "2024-04-16T03:03:21.828170Z"
    },
    "papermill": {
     "duration": 5.051881,
     "end_time": "2024-04-16T03:03:21.831731",
     "exception": false,
     "start_time": "2024-04-16T03:03:16.779850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='unsloth/mistral-7b-instruct-v0.2-bnb-4bit', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=32, target_modules={'model.layers.31.self_attn.q_proj', 'model.layers.31.self_attn.o_proj', 'model.layers.31.self_attn.k_proj', 'model.layers.31.mlp.gate_proj', 'model.layers.31.mlp.down_proj', 'model.layers.31.mlp.up_proj', 'model.layers.31.self_attn.v_proj'}, lora_alpha=128, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "#del\n",
    "del model,tokenizer\n",
    "torch.cuda.empty_cache()\n",
    "#\n",
    "MODEL_NAME = \"unsloth/mistral-7b-bnb-4bit\"\n",
    "\n",
    "PEFT_MODEL = \"/kaggle/input/mistral-promptguess2/trained-model-it-promptguess-2\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "print(config)\n",
    "\n",
    "base_model_path=\"/kaggle/input/custom-mistral-inference/unsloth\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = PeftModel.from_pretrained(model, PEFT_MODEL)\n",
    "\n",
    "\n",
    "\n",
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 80\n",
    "generation_config.temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "max_new_tokens = 30\n",
    "max_sentences_in_response = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0492864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T03:03:21.864693Z",
     "iopub.status.busy": "2024-04-16T03:03:21.864389Z",
     "iopub.status.idle": "2024-04-16T03:03:21.871002Z",
     "shell.execute_reply": "2024-04-16T03:03:21.869932Z"
    },
    "papermill": {
     "duration": 0.024591,
     "end_time": "2024-04-16T03:03:21.872935",
     "exception": false,
     "start_time": "2024-04-16T03:03:21.848344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat_template = open('/kaggle/input/mistral-instruct-templ/mistral-instruct.jinja').read()\n",
    "chat_template = chat_template.replace('    ', '').replace('\\n', '')\n",
    "tokenizer.chat_template=chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d15b7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T03:03:21.896029Z",
     "iopub.status.busy": "2024-04-16T03:03:21.895505Z",
     "iopub.status.idle": "2024-04-16T03:03:21.912798Z",
     "shell.execute_reply": "2024-04-16T03:03:21.911970Z"
    },
    "papermill": {
     "duration": 0.031202,
     "end_time": "2024-04-16T03:03:21.914790",
     "exception": false,
     "start_time": "2024-04-16T03:03:21.883588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "orig_prefix = \"Original Text:\"\n",
    "\n",
    "\n",
    "llm_response_for_rewrite = \"Provide the new text and I will tell you what new element was added or change in tone was made to improve it - with no references to the original.  I will avoid mentioning names of characters.  It is crucial no person, place or thing from the original text be mentioned.  For example - I will not say things like 'change the puppet show into a book report' - I would just say 'improve this text into a book report'.  If the original text mentions a specific idea, person, place, or thing - I will not mention it in my answer.  For example if there is a 'dog' or 'office' in the original text - the word 'dog' or 'office' must not be in my response.\"\n",
    "\n",
    "rewrite_prefix = \"Re-written Text:\"\n",
    "\n",
    "response_start = \"The request was: \"\n",
    "\n",
    "examples_sequences = [\n",
    "    (\n",
    "        \"Hey there! Just a heads up: our friendly dog may bark a bit, but don't worry, he's all bark and no bite!\",\n",
    "        \"Warning: Protective dog on premises. May exhibit aggressive behavior. Ensure personal safety by maintaining distance and avoiding direct contact.\",\n",
    "        \"Please improve the following text by reimagining it through the lens of a warning.\"\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"A lunar eclipse happens when Earth casts its shadow on the moon during a full moon. The moon appears reddish because Earth's atmosphere scatters sunlight, some of which refracts onto the moon's surface. Total eclipses see the moon entirely in Earth's shadow; partial ones occur when only part of the moon is shadowed.\",\n",
    "        \"Yo check it, when the Earth steps in, takes its place, casting shadows on the moon's face. It's a full moon night, the scene's set right, for a lunar eclipse, a celestial sight. The moon turns red, ain't no dread, it's just Earth's atmosphere playing with sunlight's thread, scattering colors, bending light, onto the moon's surface, making the night bright. Total eclipse, the moon's fully in the dark, covered by Earth's shadow, making its mark. But when it's partial, not all is shadowed, just a piece of the moon, slightly furrowed. So that's the rap, the lunar eclipse track, a dance of shadows, with no slack. Earth, moon, and sun, in a cosmic play, creating the spectacle we see today.\",\n",
    "        \"Please improve the following text by reimagining it through the lens of a rap.\"\n",
    "    ),\n",
    "    \n",
    "    (\n",
    "        \"Drinking enough water each day is crucial for many functions in the body, such as regulating temperature, keeping joints lubricated, preventing infections, delivering nutrients to cells, and keeping organs functioning properly. Being well-hydrated also improves sleep quality, cognition, and mood.\",\n",
    "        \"Arrr, crew! Sail the health seas with water, the ultimate treasure! It steadies yer body's ship, fights off plagues, and keeps yer mind sharp. Hydrate or walk the plank into the abyss of ill health. Let's hoist our bottles high and drink to the horizon of well-being!\",\n",
    "        \"Please improve the following text by reimagining it through the lens of a pirate would.\"\n",
    "    ),\n",
    "    \n",
    "    \n",
    "    (\n",
    "        \"The Johnson family planned a delightful Saturday outing at Green Meadow Park. They packed sandwiches, apples, and iced tea into their picnic basket. The children, Lily and Max, raced each other to the swing set, their laughter echoing across the park. Mr. and Mrs. Johnson followed at a leisurely pace, smiling at the joy of their children. Finding a shaded spot under a grand oak, they spread their blanket and set out the food. After eating, the children kicked a soccer ball around, while their parents lay back, soaking in the serene atmosphere.\",\n",
    "        \"Woof! Today's adventure was pawsome! My pack, the Johnsons, led us to the vast expanse of Green Meadow Park, my tail wagging in excitement. They brought the food holder – oh, how I drooled over the scent of sandwiches and apples! My little humans, Lily and Max, dashed towards the swings, and I bounded alongside, barking joyfully. My big humans meandered, allowing me to investigate every intriguing aroma. We claimed our territory under a mighty oak, the perfect spot for naps and guarding. Post-feast, it was playtime! I chased the soccer ball, ensuring my pack's laughter and safety. Life's good when you're the family dog.\",\n",
    "        \"Please improve the following text by reimagining it through the lens of a dog.\"\n",
    "    ),\n",
    "\n",
    "            \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389d55e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T03:03:21.937230Z",
     "iopub.status.busy": "2024-04-16T03:03:21.936904Z",
     "iopub.status.idle": "2024-04-16T03:03:21.950131Z",
     "shell.execute_reply": "2024-04-16T03:03:21.949323Z"
    },
    "papermill": {
     "duration": 0.026857,
     "end_time": "2024-04-16T03:03:21.952000",
     "exception": false,
     "start_time": "2024-04-16T03:03:21.925143",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_numbered_list(text):\n",
    "    final_text_paragraphs = [] \n",
    "    for line in text.split('\\n'):\n",
    "        \n",
    "        parts = line.split('. ', 1)\n",
    "        \n",
    "        if len(parts) > 1 and parts[0].isdigit():\n",
    "            final_text_paragraphs.append(parts[1].strip())\n",
    "        else:\n",
    "            final_text_paragraphs.append(line.strip())\n",
    "\n",
    "    return ' '.join(final_text_paragraphs)\n",
    "\n",
    "\n",
    "\n",
    "def trim_to_response(text):\n",
    "    terminate_string = \"[/INST]\"\n",
    "    text = text.replace('</s>', '')\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "\n",
    "    last_pos = text.rfind(terminate_string)\n",
    "    return text[last_pos + len(terminate_string):] if last_pos != -1 else text\n",
    "\n",
    "\n",
    "def extract_text_after_response_start(full_text):\n",
    "    parts = full_text.rsplit(response_start, 1)\n",
    "    if len(parts) > 1:\n",
    "        return parts[1].strip()\n",
    "    else:\n",
    "        return full_text  \n",
    "    \n",
    "\n",
    "def trim_to_first_x_sentences_or_lf(text, x):\n",
    "    if x <= 0:\n",
    "        return \"\"\n",
    "    text = text.replace(\"  \", \"\\n\")\n",
    "\n",
    "    text_chunks = text.split('\\n')\n",
    "    if len(text_chunks)>1:\n",
    "        return text_chunks[0]+\" \" + text_chunks[1]\n",
    "    else:\n",
    "        return text_chunks[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be7e78c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T03:03:21.973945Z",
     "iopub.status.busy": "2024-04-16T03:03:21.973688Z",
     "iopub.status.idle": "2024-04-16T03:03:21.979084Z",
     "shell.execute_reply": "2024-04-16T03:03:21.978248Z"
    },
    "papermill": {
     "duration": 0.018952,
     "end_time": "2024-04-16T03:03:21.981389",
     "exception": false,
     "start_time": "2024-04-16T03:03:21.962437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def actually_remove_numbering(text):\n",
    "    res=text\n",
    "    for num in [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]:\n",
    "        numbering=num+\".\"\n",
    "        res=res.replace(numbering,\"\")\n",
    "    for num in [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]:\n",
    "        res=res.replace(num,\"\")\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01f0854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T03:03:22.005352Z",
     "iopub.status.busy": "2024-04-16T03:03:22.005080Z",
     "iopub.status.idle": "2024-04-16T03:03:22.014422Z",
     "shell.execute_reply": "2024-04-16T03:03:22.013654Z"
    },
    "papermill": {
     "duration": 0.02257,
     "end_time": "2024-04-16T03:03:22.016260",
     "exception": false,
     "start_time": "2024-04-16T03:03:21.993690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prompt(orig_text, transformed_text):\n",
    "\n",
    "    messages = []\n",
    "    messages=[{\"role\":\"system\", \"content\": sys_mes_prompt_rewrite}]\n",
    "    \n",
    "    for example_text, example_rewrite, example_prompt in examples_sequences:\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"{orig_prefix} {example_text}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llm_response_for_rewrite})\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"{rewrite_prefix} {example_rewrite}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"{response_start} {example_prompt}\"})\n",
    "\n",
    "    #actual prompt\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{orig_prefix} {orig_text}\"})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": llm_response_for_rewrite})\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{rewrite_prefix} {transformed_text}\"})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"{response_start} {response_prefix}\"})\n",
    "        \n",
    "    #give it to Mistral\n",
    "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    model_inputs = model_inputs.to(\"cuda\") \n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    \n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    \n",
    "    just_response = trim_to_response(decoded[0])        \n",
    "    \n",
    "    \n",
    "    final_text = extract_text_after_response_start(just_response)\n",
    "    final_text = remove_numbered_list(final_text)\n",
    "    final_text=actually_remove_numbering(final_text)\n",
    "    final_text = trim_to_first_x_sentences_or_lf(final_text, max_sentences_in_response)\n",
    "    \n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1e80f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T03:03:22.038211Z",
     "iopub.status.busy": "2024-04-16T03:03:22.037929Z",
     "iopub.status.idle": "2024-04-16T03:03:22.042761Z",
     "shell.execute_reply": "2024-04-16T03:03:22.041940Z"
    },
    "papermill": {
     "duration": 0.017986,
     "end_time": "2024-04-16T03:03:22.044631",
     "exception": false,
     "start_time": "2024-04-16T03:03:22.026645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def combine_prompt(prompt):\n",
    "    res=prompt.rstrip(\".\") + response_suffix\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31cde559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T03:03:22.067966Z",
     "iopub.status.busy": "2024-04-16T03:03:22.067709Z",
     "iopub.status.idle": "2024-04-16T03:08:39.021379Z",
     "shell.execute_reply": "2024-04-16T03:08:39.020606Z"
    },
    "papermill": {
     "duration": 316.96695,
     "end_time": "2024-04-16T03:08:39.023559",
     "exception": false,
     "start_time": "2024-04-16T03:03:22.056609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "for index, row in test_df.iterrows():\n",
    "    if row['done']==True:\n",
    "        continue\n",
    "    else:\n",
    "        result = get_prompt(row['original_text'], row['rewritten_text'])\n",
    "        result = result.rstrip('.') + response_suffix\n",
    "        test_df.at[index, 'rewrite_prompt'] = result\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b19d395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T03:08:39.049130Z",
     "iopub.status.busy": "2024-04-16T03:08:39.048834Z",
     "iopub.status.idle": "2024-04-16T03:08:39.078600Z",
     "shell.execute_reply": "2024-04-16T03:08:39.077754Z"
    },
    "papermill": {
     "duration": 0.04493,
     "end_time": "2024-04-16T03:08:39.080805",
     "exception": false,
     "start_time": "2024-04-16T03:08:39.035875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1419                                                                                                                                                                                                                                                                                                      Please improve this text to be a creative storytelling using the writing style while maintaining the original meaning but altering the tone.\n",
      "556                                                                                                                                                                                                                                                                                          Please improve this text using the writing style, maintaining the original meaning but altering the tone to expound on its significance in a poetic form.\n",
      "1879                                                                                             Please improve the following text by reimagining it through the lens of a more positive and hopeful tone, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "1430                                                                                                                                                                                                                                                                                             Please improve this text using the writing style, maintaining the original meaning but altering the tone and rearranging it into a newsletter format.\n",
      "2370                                                                                     Please improve the following text by reimagining it through the lens of a speaking in a dramatic and poetic tone, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "2                                                                                                           Please improve the following text by reimagining it through the lens of a Star Wars character, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "819                                                                                     Please improve the following text by reimagining it through the lens of a European fairy tale with a darker twist, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "2341                                                                                                    Please improve the following text by reimagining it through the lens of a old, grizzled detective, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "1862    Please improve the following text by reimagining it through the lens of a mystery or a science fiction story. In the original text, the narrator is dealing with the loss of their mother and the, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "940                                                                                                      Please improve the following text by reimagining it through the lens of a first person narrative, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "1868                                                                                                                       Please improve the following text by reimagining it through the lens of a poet, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "630                                                                                                                                                                                                                                                                                                       Please improve this text to be a creative storytelling using the writing style while maintaining the original meaning but altering the tone.\n",
      "1456                                                                                                   Please improve the following text by reimagining it through the lens of a thriller or horror genre, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "343                                                                                                     Please improve the following text by reimagining it through the lens of a feudal Japanese warrior, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "194                                                                                                            Please improve the following text by reimagining it through the lens of a old, fearful man, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "2239                                                                                                        Please improve the following text by reimagining it through the lens of a gothic horror story, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "963                                                                                                         Please improve the following text by reimagining it through the lens of a European fairy tale, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "1328                                                                                                                                                                                                                                                                                                      Please improve this text to be a creative storytelling using the writing style while maintaining the original meaning but altering the tone.\n",
      "2138                                                                                                Please improve the following text by reimagining it through the lens of a s film noir detective story, retaining the original essence while elevating its clarity, eloquence, and potency by modulating the tone, word choice, and stylistic nuances to harmoniously embody the stylistic features while ensuring the core message remains intact.\n",
      "483                                                                                                                                                                                                                                                                                          Please improve this text using the writing style, maintaining the original meaning but altering the tone to expound on its significance in a poetic form.\n",
      "Name: rewrite_prompt, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    test_df = test_df[['id', 'rewrite_prompt']]\n",
    "    test_df.to_csv('submission.csv', index=False)\n",
    "else:\n",
    "    test_df.to_csv('submission.csv', index=False)\n",
    "    testdf_copy.to_csv('copy.csv')\n",
    "    print(test_df['rewrite_prompt'].head(20))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7806901,
     "sourceId": 67121,
     "sourceType": "competition"
    },
    {
     "datasetId": 4506214,
     "sourceId": 7747717,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4759514,
     "sourceId": 8067108,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4761229,
     "sourceId": 8069558,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4778962,
     "sourceId": 8094656,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4792017,
     "sourceId": 8111944,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4760998,
     "sourceId": 8119554,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4804763,
     "sourceId": 8129302,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4804768,
     "sourceId": 8129309,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 171703816,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 172178518,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 787.258203,
   "end_time": "2024-04-16T03:08:42.015676",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-16T02:55:34.757473",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
